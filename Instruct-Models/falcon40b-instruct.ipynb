{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/varad/snap/snapd-desktop-integration/83/Desktop/Falcon/falconllmenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model = \"tiiuae/falcon-40b-instruct\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "Downloading (…)l-00005-of-00009.bin: 100%|██████████| 9.51G/9.51G [06:00<00:00, 26.4MB/s]\n",
      "Downloading (…)l-00006-of-00009.bin: 100%|██████████| 9.51G/9.51G [05:15<00:00, 30.2MB/s]\n",
      "Downloading (…)l-00007-of-00009.bin: 100%|██████████| 9.51G/9.51G [04:51<00:00, 32.7MB/s]\n",
      "Downloading (…)l-00008-of-00009.bin: 100%|██████████| 9.51G/9.51G [05:27<00:00, 29.1MB/s]\n",
      "Downloading (…)l-00009-of-00009.bin: 100%|██████████| 7.58G/7.58G [03:19<00:00, 38.0MB/s]\n",
      "Downloading shards: 100%|██████████| 9/9 [29:43<00:00, 198.16s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 9/9 [03:52<00:00, 25.85s/it]\n",
      "Downloading generation_config.json: 100%|██████████| 117/117 [00:00<00:00, 497kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "prompt = \"Write few lines about LLM finetuning\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sequences = pipeline(\n",
    "   prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Result: Write few lines about LLM finetuning\n",
      "LLM finetuning is a technique of training a language model on a specific task or domain by feeding it with a large quantity of relevant texts to understand the context and nuances of that domain. For example, if one wanted to finetune a language model to be used as a medical assistant, one would feed it with large quantities of clinical notes, medical articles, and medical textbooks to help it understand the specific vocabulary and concepts of medicine. This process helps the language model to improve its accuracy and relevance in the target domain, and it is often used to make the model more efficient in handling real-world tasks.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.12 64-bit ('falconllmenv': venv)"
  },
  "interpreter": {
   "hash": "fbe531172e303b58e4deaed90ac102adfea86e618bd9090930f5ad915745890a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}